#!/usr/bin/env python3
"""
æ•°æ®é›†ä¸‹è½½è„šæœ¬

ä¸‹è½½å’Œå‡†å¤‡æ ‡å‡†RAGè¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…æ‹¬ï¼š
1. MS MARCO - å¾®è½¯å¤§è§„æ¨¡é—®ç­”æ•°æ®é›†
2. Natural Questions - Googleè‡ªç„¶é—®é¢˜æ•°æ®é›†  
3. BEIR - ä¿¡æ¯æ£€ç´¢åŸºå‡†æ•°æ®é›†
4. HotpotQA - å¤šè·³æ¨ç†é—®ç­”æ•°æ®é›†
5. FiQA - é‡‘èé—®ç­”æ•°æ®é›†
"""

import os
import sys
import json
import gzip
import requests
import zipfile
import tarfile
from pathlib import Path
from typing import Dict, List, Optional
import logging
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.logging import setup_logger

class DatasetDownloader:
    """æ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        """
        åˆå§‹åŒ–æ•°æ®é›†ä¸‹è½½å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.logger = setup_logger("DatasetDownloader")
        
        # æ•°æ®é›†é…ç½®
        self.datasets_config = {
            "ms_marco": {
                "name": "MS MARCO Passage Ranking",
                "description": "å¾®è½¯å¤§è§„æ¨¡æ®µè½æ’åºæ•°æ®é›†",
                "urls": {
                    "queries_train": "https://msmarco.blob.core.windows.net/msmarcoranking/queries.train.tsv",
                    "queries_dev": "https://msmarco.blob.core.windows.net/msmarcoranking/queries.dev.tsv",
                    "collection": "https://msmarco.blob.core.windows.net/msmarcoranking/collection.tsv",
                    "qrels_train": "https://msmarco.blob.core.windows.net/msmarcoranking/qrels.train.tsv",
                    "qrels_dev": "https://msmarco.blob.core.windows.net/msmarcoranking/qrels.dev.tsv"
                },
                "size": "~3.2GB"
            },
            "natural_questions": {
                "name": "Natural Questions",
                "description": "Googleè‡ªç„¶é—®é¢˜æ•°æ®é›†",
                "urls": {
                    "train": "https://storage.googleapis.com/natural_questions/v1.0-simplified/simplified-nq-train.jsonl.gz",
                    "dev": "https://storage.googleapis.com/natural_questions/v1.0-simplified/nq-dev-all.jsonl.gz"
                },
                "size": "~1.1GB"
            },
            "hotpot_qa": {
                "name": "HotpotQA",
                "description": "å¤šè·³æ¨ç†é—®ç­”æ•°æ®é›†",
                "urls": {
                    "train": "http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json",
                    "dev": "http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_distractor_v1.json",
                    "test": "http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_test_fullwiki_v1.json"
                },
                "size": "~500MB"
            },
            "fiqa": {
                "name": "FiQA",
                "description": "é‡‘èé—®ç­”æ•°æ®é›†",
                "urls": {
                    "train": "https://sites.google.com/view/fiqa/",
                    "test": "https://sites.google.com/view/fiqa/"
                },
                "size": "~50MB",
                "note": "éœ€è¦æ‰‹åŠ¨ä¸‹è½½"
            }
        }
    
    def download_file(self, url: str, filepath: Path, chunk_size: int = 8192) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶
        
        Args:
            url: ä¸‹è½½é“¾æ¥
            filepath: ä¿å­˜è·¯å¾„
            chunk_size: å—å¤§å°
            
        Returns:
            bool: æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            self.logger.info(f"å¼€å§‹ä¸‹è½½: {url}")
            
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, 'wb') as f:
                with tqdm(total=total_size, unit='B', unit_scale=True, desc=filepath.name) as pbar:
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))
            
            self.logger.info(f"ä¸‹è½½å®Œæˆ: {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {str(e)}")
            return False
    
    def extract_archive(self, filepath: Path) -> bool:
        """
        è§£å‹æ–‡ä»¶
        
        Args:
            filepath: å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦è§£å‹æˆåŠŸ
        """
        try:
            extract_dir = filepath.parent / filepath.stem
            
            if filepath.suffix == '.gz':
                if filepath.stem.endswith('.tar'):
                    # tar.gzæ–‡ä»¶
                    with tarfile.open(filepath, 'r:gz') as tar:
                        tar.extractall(extract_dir)
                else:
                    # .jsonl.gzç­‰æ–‡ä»¶
                    with gzip.open(filepath, 'rb') as f_in:
                        with open(extract_dir.with_suffix(''), 'wb') as f_out:
                            f_out.write(f_in.read())
            elif filepath.suffix == '.zip':
                with zipfile.ZipFile(filepath, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
            
            self.logger.info(f"è§£å‹å®Œæˆ: {extract_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"è§£å‹å¤±è´¥ {filepath}: {str(e)}")
            return False
    
    def download_ms_marco(self) -> bool:
        """ä¸‹è½½MS MARCOæ•°æ®é›†"""
        self.logger.info("å¼€å§‹ä¸‹è½½MS MARCOæ•°æ®é›†...")
        
        dataset_dir = self.data_dir / "ms_marco"
        dataset_dir.mkdir(exist_ok=True)
        
        config = self.datasets_config["ms_marco"]
        success = True
        
        for name, url in config["urls"].items():
            filepath = dataset_dir / f"{name}.tsv"
            
            if filepath.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filepath}")
                continue
            
            if not self.download_file(url, filepath):
                success = False
        
        return success
    
    def download_natural_questions(self) -> bool:
        """ä¸‹è½½Natural Questionsæ•°æ®é›†"""
        self.logger.info("å¼€å§‹ä¸‹è½½Natural Questionsæ•°æ®é›†...")
        
        dataset_dir = self.data_dir / "natural_questions"
        dataset_dir.mkdir(exist_ok=True)
        
        config = self.datasets_config["natural_questions"]
        success = True
        
        for name, url in config["urls"].items():
            filename = url.split('/')[-1]
            filepath = dataset_dir / filename
            
            if filepath.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filepath}")
                continue
            
            if self.download_file(url, filepath):
                # è§£å‹.gzæ–‡ä»¶
                if filepath.suffix == '.gz':
                    self.extract_archive(filepath)
            else:
                success = False
        
        return success
    
    def download_hotpot_qa(self) -> bool:
        """ä¸‹è½½HotpotQAæ•°æ®é›†"""
        self.logger.info("å¼€å§‹ä¸‹è½½HotpotQAæ•°æ®é›†...")
        
        dataset_dir = self.data_dir / "hotpot_qa"
        dataset_dir.mkdir(exist_ok=True)
        
        config = self.datasets_config["hotpot_qa"]
        success = True
        
        for name, url in config["urls"].items():
            filename = url.split('/')[-1]
            filepath = dataset_dir / filename
            
            if filepath.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filepath}")
                continue
            
            if not self.download_file(url, filepath):
                success = False
        
        return success
    
    def create_sample_datasets(self):
        """åˆ›å»ºå°æ ·æœ¬æ•°æ®é›†ç”¨äºå¿«é€Ÿæµ‹è¯•"""
        self.logger.info("åˆ›å»ºæ ·æœ¬æ•°æ®é›†...")
        
        sample_dir = self.data_dir / "samples"
        sample_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ ·æœ¬æŸ¥è¯¢æ•°æ®
        sample_queries = [
            {"id": "1", "query": "What is machine learning?", "type": "factual"},
            {"id": "2", "query": "How do neural networks work?", "type": "technical"},
            {"id": "3", "query": "Compare supervised and unsupervised learning", "type": "comparison"},
            {"id": "4", "query": "What are the applications of deep learning?", "type": "application"},
            {"id": "5", "query": "Explain the transformer architecture", "type": "detailed"}
        ]
        
        # åˆ›å»ºæ ·æœ¬æ–‡æ¡£æ•°æ®
        sample_documents = [
            {"id": "doc_1", "title": "Machine Learning Basics", 
             "content": "Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed."},
            {"id": "doc_2", "title": "Neural Networks", 
             "content": "Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes that process information."},
            {"id": "doc_3", "title": "Supervised Learning", 
             "content": "Supervised learning uses labeled training data to learn a mapping from inputs to outputs."},
            {"id": "doc_4", "title": "Deep Learning Applications", 
             "content": "Deep learning has applications in computer vision, natural language processing, speech recognition, and many other fields."},
            {"id": "doc_5", "title": "Transformer Architecture", 
             "content": "The transformer is a neural network architecture that relies entirely on attention mechanisms to draw global dependencies."}
        ]
        
        # ä¿å­˜æ ·æœ¬æ•°æ®
        with open(sample_dir / "queries.json", 'w', encoding='utf-8') as f:
            json.dump(sample_queries, f, indent=2, ensure_ascii=False)
        
        with open(sample_dir / "documents.json", 'w', encoding='utf-8') as f:
            json.dump(sample_documents, f, indent=2, ensure_ascii=False)
        
        self.logger.info("æ ·æœ¬æ•°æ®é›†åˆ›å»ºå®Œæˆ")
    
    def download_all(self, datasets: Optional[List[str]] = None):
        """
        ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        
        Args:
            datasets: è¦ä¸‹è½½çš„æ•°æ®é›†åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä¸‹è½½æ‰€æœ‰
        """
        if datasets is None:
            datasets = ["ms_marco", "natural_questions", "hotpot_qa"]
        
        self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {datasets}")
        
        # åˆ›å»ºæ ·æœ¬æ•°æ®é›†
        self.create_sample_datasets()
        
        # ä¸‹è½½æŒ‡å®šæ•°æ®é›†
        download_methods = {
            "ms_marco": self.download_ms_marco,
            "natural_questions": self.download_natural_questions,
            "hotpot_qa": self.download_hotpot_qa
        }
        
        results = {}
        for dataset in datasets:
            if dataset in download_methods:
                self.logger.info(f"\n{'='*50}")
                self.logger.info(f"ä¸‹è½½æ•°æ®é›†: {self.datasets_config[dataset]['name']}")
                self.logger.info(f"æè¿°: {self.datasets_config[dataset]['description']}")
                self.logger.info(f"å¤§å°: {self.datasets_config[dataset]['size']}")
                self.logger.info(f"{'='*50}")
                
                results[dataset] = download_methods[dataset]()
            else:
                self.logger.warning(f"æœªçŸ¥æ•°æ®é›†: {dataset}")
                results[dataset] = False
        
        # è¾“å‡ºä¸‹è½½ç»“æœ
        self.logger.info(f"\n{'='*50}")
        self.logger.info("ä¸‹è½½ç»“æœæ€»ç»“:")
        for dataset, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            self.logger.info(f"  {dataset}: {status}")
        self.logger.info(f"{'='*50}")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¸‹è½½RAGè¯„ä¼°æ•°æ®é›†")
    parser.add_argument("--datasets", nargs="+", 
                       choices=["ms_marco", "natural_questions", "hotpot_qa", "all"],
                       default=["all"],
                       help="è¦ä¸‹è½½çš„æ•°æ®é›†")
    parser.add_argument("--data-dir", default="data", 
                       help="æ•°æ®å­˜å‚¨ç›®å½•")
    parser.add_argument("--sample-only", action="store_true",
                       help="åªåˆ›å»ºæ ·æœ¬æ•°æ®é›†")
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = DatasetDownloader(args.data_dir)
    
    if args.sample_only:
        downloader.create_sample_datasets()
        print("âœ… æ ·æœ¬æ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
        return
    
    # å¤„ç†æ•°æ®é›†åˆ—è¡¨
    if "all" in args.datasets:
        datasets = ["ms_marco", "natural_questions", "hotpot_qa"]
    else:
        datasets = args.datasets
    
    # ä¸‹è½½æ•°æ®é›†
    results = downloader.download_all(datasets)
    
    # è¾“å‡ºç»“æœ
    success_count = sum(results.values())
    total_count = len(results)
    
    if success_count == total_count:
        print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½æˆåŠŸï¼({success_count}/{total_count})")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æ•°æ®é›†ä¸‹è½½å¤±è´¥ ({success_count}/{total_count})")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡è¯•å¤±è´¥çš„æ•°æ®é›†")


if __name__ == "__main__":
    main()
