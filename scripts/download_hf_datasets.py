#!/usr/bin/env python3
"""
HuggingFaceæ•°æ®é›†ä¸‹è½½è„šæœ¬

ä½¿ç”¨HuggingFace datasetsåº“ä¸‹è½½æ ‡å‡†RAGè¯„ä¼°æ•°æ®é›†ï¼Œ
è¿™æ˜¯æ›´å¯é çš„æ•°æ®è·å–æ–¹å¼ã€‚
"""

import os
import sys
import json
from pathlib import Path
from typing import Dict, List, Optional
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.logging import setup_logger


class HFDatasetDownloader:
    """HuggingFaceæ•°æ®é›†ä¸‹è½½å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        """
        åˆå§‹åŒ–æ•°æ®é›†ä¸‹è½½å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.logger = setup_logger("HFDatasetDownloader")
        
        # æ£€æŸ¥datasetsåº“
        try:
            import datasets
            self.datasets = datasets
            self.logger.info("HuggingFace datasetsåº“å·²åŠ è½½")
        except ImportError:
            self.logger.error("è¯·å®‰è£…datasetsåº“: pip install datasets")
            sys.exit(1)
    
    def download_ms_marco(self, subset_size: int = 1000) -> bool:
        """
        ä¸‹è½½MS MARCOæ•°æ®é›†çš„å­é›†
        
        Args:
            subset_size: å­é›†å¤§å°
            
        Returns:
            bool: æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            self.logger.info("å¼€å§‹ä¸‹è½½MS MARCOæ•°æ®é›†...")
            
            # åˆ›å»ºæ•°æ®ç›®å½•
            dataset_dir = self.data_dir / "ms_marco"
            dataset_dir.mkdir(exist_ok=True)
            
            # ä¸‹è½½MS MARCOæ•°æ®é›†
            dataset = self.datasets.load_dataset("ms_marco", "v1.1", split="train")
            
            # å–å­é›†
            if len(dataset) > subset_size:
                dataset = dataset.select(range(subset_size))
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            queries = []
            documents = []
            
            for i, example in enumerate(dataset):
                # æŸ¥è¯¢
                query = {
                    "id": f"msmarco_{i}",
                    "query": example["query"],
                    "type": "factual"
                }
                queries.append(query)
                
                # æ–‡æ¡£ (ä½¿ç”¨passages)
                for j, passage in enumerate(example["passages"]["passage_text"]):
                    if passage.strip():  # è¿‡æ»¤ç©ºæ–‡æ¡£
                        doc = {
                            "id": f"msmarco_doc_{i}_{j}",
                            "title": f"MS MARCO Document {i}-{j}",
                            "content": passage.strip(),
                            "source": "ms_marco"
                        }
                        documents.append(doc)
            
            # ä¿å­˜æ•°æ®
            with open(dataset_dir / "queries.json", 'w', encoding='utf-8') as f:
                json.dump(queries, f, indent=2, ensure_ascii=False)
            
            with open(dataset_dir / "documents.json", 'w', encoding='utf-8') as f:
                json.dump(documents, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"MS MARCOæ•°æ®é›†ä¸‹è½½å®Œæˆ: {len(queries)}ä¸ªæŸ¥è¯¢, {len(documents)}ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            self.logger.error(f"MS MARCOæ•°æ®é›†ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def download_natural_questions(self, subset_size: int = 500) -> bool:
        """
        ä¸‹è½½Natural Questionsæ•°æ®é›†çš„å­é›†
        
        Args:
            subset_size: å­é›†å¤§å°
            
        Returns:
            bool: æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            self.logger.info("å¼€å§‹ä¸‹è½½Natural Questionsæ•°æ®é›†...")
            
            # åˆ›å»ºæ•°æ®ç›®å½•
            dataset_dir = self.data_dir / "natural_questions"
            dataset_dir.mkdir(exist_ok=True)
            
            # ä¸‹è½½Natural Questionsæ•°æ®é›†
            dataset = self.datasets.load_dataset("natural_questions", split="train")
            
            # å–å­é›†
            if len(dataset) > subset_size:
                dataset = dataset.select(range(subset_size))
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            queries = []
            documents = []
            
            for i, example in enumerate(dataset):
                # æŸ¥è¯¢
                query = {
                    "id": f"nq_{i}",
                    "query": example["question"]["text"],
                    "type": "factual"
                }
                queries.append(query)
                
                # æ–‡æ¡£ (ä½¿ç”¨document_text)
                if example["document"]["tokens"]["token"]:
                    # é‡æ„æ–‡æ¡£æ–‡æœ¬
                    tokens = example["document"]["tokens"]["token"]
                    content = " ".join(tokens[:500])  # é™åˆ¶é•¿åº¦
                    
                    doc = {
                        "id": f"nq_doc_{i}",
                        "title": example["document"]["title"] or f"NQ Document {i}",
                        "content": content,
                        "source": "natural_questions"
                    }
                    documents.append(doc)
            
            # ä¿å­˜æ•°æ®
            with open(dataset_dir / "queries.json", 'w', encoding='utf-8') as f:
                json.dump(queries, f, indent=2, ensure_ascii=False)
            
            with open(dataset_dir / "documents.json", 'w', encoding='utf-8') as f:
                json.dump(documents, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"Natural Questionsæ•°æ®é›†ä¸‹è½½å®Œæˆ: {len(queries)}ä¸ªæŸ¥è¯¢, {len(documents)}ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            self.logger.error(f"Natural Questionsæ•°æ®é›†ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def download_squad(self, subset_size: int = 1000) -> bool:
        """
        ä¸‹è½½SQuADæ•°æ®é›†ä½œä¸ºæ›¿ä»£
        
        Args:
            subset_size: å­é›†å¤§å°
            
        Returns:
            bool: æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            self.logger.info("å¼€å§‹ä¸‹è½½SQuADæ•°æ®é›†...")
            
            # åˆ›å»ºæ•°æ®ç›®å½•
            dataset_dir = self.data_dir / "squad"
            dataset_dir.mkdir(exist_ok=True)
            
            # ä¸‹è½½SQuADæ•°æ®é›†
            dataset = self.datasets.load_dataset("squad", split="train")
            
            # å–å­é›†
            if len(dataset) > subset_size:
                dataset = dataset.select(range(subset_size))
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            queries = []
            documents = []
            
            for i, example in enumerate(dataset):
                # æŸ¥è¯¢
                query = {
                    "id": f"squad_{i}",
                    "query": example["question"],
                    "type": "factual",
                    "answer": example["answers"]["text"][0] if example["answers"]["text"] else ""
                }
                queries.append(query)
                
                # æ–‡æ¡£
                doc = {
                    "id": f"squad_doc_{i}",
                    "title": example["title"],
                    "content": example["context"],
                    "source": "squad"
                }
                documents.append(doc)
            
            # ä¿å­˜æ•°æ®
            with open(dataset_dir / "queries.json", 'w', encoding='utf-8') as f:
                json.dump(queries, f, indent=2, ensure_ascii=False)
            
            with open(dataset_dir / "documents.json", 'w', encoding='utf-8') as f:
                json.dump(documents, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"SQuADæ•°æ®é›†ä¸‹è½½å®Œæˆ: {len(queries)}ä¸ªæŸ¥è¯¢, {len(documents)}ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            self.logger.error(f"SQuADæ•°æ®é›†ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def create_synthetic_dataset(self, size: int = 100) -> bool:
        """
        åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºå¿«é€Ÿæµ‹è¯•
        
        Args:
            size: æ•°æ®é›†å¤§å°
            
        Returns:
            bool: æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        try:
            self.logger.info("åˆ›å»ºåˆæˆæ•°æ®é›†...")
            
            # åˆ›å»ºæ•°æ®ç›®å½•
            dataset_dir = self.data_dir / "synthetic"
            dataset_dir.mkdir(exist_ok=True)
            
            # åˆæˆæŸ¥è¯¢æ¨¡æ¿
            query_templates = [
                "What is {topic}?",
                "How does {topic} work?",
                "What are the benefits of {topic}?",
                "Compare {topic} with alternatives",
                "Explain the concept of {topic}",
                "What are the applications of {topic}?",
                "How to implement {topic}?",
                "What are the challenges in {topic}?",
                "Future trends in {topic}",
                "Best practices for {topic}"
            ]
            
            # ä¸»é¢˜åˆ—è¡¨
            topics = [
                "machine learning", "deep learning", "neural networks", "artificial intelligence",
                "natural language processing", "computer vision", "reinforcement learning",
                "transformer models", "attention mechanism", "BERT", "GPT", "retrieval systems",
                "information retrieval", "question answering", "text summarization",
                "sentiment analysis", "named entity recognition", "part-of-speech tagging",
                "semantic search", "vector databases", "embedding models", "fine-tuning",
                "transfer learning", "few-shot learning", "zero-shot learning", "meta-learning"
            ]
            
            # ç”ŸæˆæŸ¥è¯¢å’Œæ–‡æ¡£
            queries = []
            documents = []
            
            for i in range(size):
                topic = topics[i % len(topics)]
                template = query_templates[i % len(query_templates)]
                
                # æŸ¥è¯¢
                query = {
                    "id": f"synthetic_{i}",
                    "query": template.format(topic=topic),
                    "type": "synthetic"
                }
                queries.append(query)
                
                # æ–‡æ¡£
                doc = {
                    "id": f"synthetic_doc_{i}",
                    "title": f"Introduction to {topic.title()}",
                    "content": f"{topic.title()} is an important concept in artificial intelligence and machine learning. It involves various techniques and methodologies that are widely used in modern applications. Understanding {topic} is crucial for developing effective AI systems.",
                    "source": "synthetic"
                }
                documents.append(doc)
            
            # ä¿å­˜æ•°æ®
            with open(dataset_dir / "queries.json", 'w', encoding='utf-8') as f:
                json.dump(queries, f, indent=2, ensure_ascii=False)
            
            with open(dataset_dir / "documents.json", 'w', encoding='utf-8') as f:
                json.dump(documents, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"åˆæˆæ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(queries)}ä¸ªæŸ¥è¯¢, {len(documents)}ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆæˆæ•°æ®é›†åˆ›å»ºå¤±è´¥: {str(e)}")
            return False
    
    def download_all(self, datasets: Optional[List[str]] = None, subset_size: int = 500):
        """
        ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        
        Args:
            datasets: è¦ä¸‹è½½çš„æ•°æ®é›†åˆ—è¡¨
            subset_size: æ¯ä¸ªæ•°æ®é›†çš„å­é›†å¤§å°
        """
        if datasets is None:
            datasets = ["squad", "synthetic"]  # ä½¿ç”¨æ›´å¯é çš„æ•°æ®é›†
        
        self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {datasets}")
        
        download_methods = {
            "ms_marco": lambda: self.download_ms_marco(subset_size),
            "natural_questions": lambda: self.download_natural_questions(subset_size),
            "squad": lambda: self.download_squad(subset_size),
            "synthetic": lambda: self.create_synthetic_dataset(subset_size)
        }
        
        results = {}
        for dataset in datasets:
            if dataset in download_methods:
                self.logger.info(f"\n{'='*50}")
                self.logger.info(f"å¤„ç†æ•°æ®é›†: {dataset}")
                self.logger.info(f"{'='*50}")
                
                results[dataset] = download_methods[dataset]()
            else:
                self.logger.warning(f"æœªçŸ¥æ•°æ®é›†: {dataset}")
                results[dataset] = False
        
        # è¾“å‡ºä¸‹è½½ç»“æœ
        self.logger.info(f"\n{'='*50}")
        self.logger.info("ä¸‹è½½ç»“æœæ€»ç»“:")
        for dataset, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            self.logger.info(f"  {dataset}: {status}")
        self.logger.info(f"{'='*50}")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¸‹è½½HuggingFace RAGè¯„ä¼°æ•°æ®é›†")
    parser.add_argument("--datasets", nargs="+", 
                       choices=["ms_marco", "natural_questions", "squad", "synthetic", "all"],
                       default=["squad", "synthetic"],
                       help="è¦ä¸‹è½½çš„æ•°æ®é›†")
    parser.add_argument("--data-dir", default="data", 
                       help="æ•°æ®å­˜å‚¨ç›®å½•")
    parser.add_argument("--subset-size", type=int, default=500,
                       help="æ¯ä¸ªæ•°æ®é›†çš„å­é›†å¤§å°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = HFDatasetDownloader(args.data_dir)
    
    # å¤„ç†æ•°æ®é›†åˆ—è¡¨
    if "all" in args.datasets:
        datasets = ["squad", "synthetic"]  # ä½¿ç”¨å¯é çš„æ•°æ®é›†
    else:
        datasets = args.datasets
    
    # ä¸‹è½½æ•°æ®é›†
    results = downloader.download_all(datasets, args.subset_size)
    
    # è¾“å‡ºç»“æœ
    success_count = sum(results.values())
    total_count = len(results)
    
    if success_count == total_count:
        print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½æˆåŠŸï¼({success_count}/{total_count})")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æ•°æ®é›†ä¸‹è½½å¤±è´¥ ({success_count}/{total_count})")


if __name__ == "__main__":
    main()
