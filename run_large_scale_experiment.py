#!/usr/bin/env python3
"""
å¤§è§„æ¨¡å®éªŒè„šæœ¬

è¿è¡Œæ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿçš„å¤§è§„æ¨¡å®éªŒï¼ŒåŒ…æ‹¬ï¼š
1. æ›´å¤§æ ·æœ¬é‡çš„å®éªŒ (100-500æŸ¥è¯¢)
2. æ€§èƒ½åŸºå‡†æµ‹è¯•
3. è¯¦ç»†çš„æ€§èƒ½åˆ†æ
4. ä¸ç†è®ºé¢„æœŸçš„å¯¹æ¯”
"""

import os
import sys
import json
import time
import statistics
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿ - å¤§è§„æ¨¡å®éªŒ")
    print("=" * 70)
    
    # æ£€æŸ¥ç¯å¢ƒ
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒå’Œä¾èµ–...")
    try:
        from src.core.intelligent_adapter import IntelligentAdaptiveRAG
        from experiments.experiment_runner import ExperimentRunner
        
        # æ£€æŸ¥æ–°å®‰è£…çš„ä¾èµ–
        try:
            import sentence_transformers
            import faiss
            print("âœ… sentence-transformers å’Œ FAISS å·²å®‰è£…")
        except ImportError:
            print("âš ï¸  sentence-transformers æˆ– FAISS æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
        
        # æ£€æŸ¥æ•°æ®é›†
        data_dir = Path("data")
        available_datasets = []
        total_queries = 0
        
        for dataset in ["samples", "squad", "synthetic"]:
            dataset_path = data_dir / dataset / "queries.json"
            if dataset_path.exists():
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    queries = json.load(f)
                available_datasets.append(dataset)
                total_queries += len(queries)
                print(f"  âœ… {dataset}: {len(queries)}ä¸ªæŸ¥è¯¢")
            else:
                print(f"  âŒ {dataset}: æ•°æ®é›†ä¸å­˜åœ¨")
        
        print(f"âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œæ€»è®¡ {total_queries} ä¸ªæŸ¥è¯¢å¯ç”¨")
        
    except ImportError as e:
        print(f"âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
        return 1
    
    # å¤§è§„æ¨¡å®éªŒé…ç½®
    experiment_configs = [
        {
            "name": "å°è§„æ¨¡åŸºå‡†æµ‹è¯•",
            "description": "éªŒè¯ç³»ç»ŸåŸºç¡€æ€§èƒ½",
            "sample_size": 10,
            "datasets": available_datasets,
            "repetitions": 1
        },
        {
            "name": "ä¸­ç­‰è§„æ¨¡æ€§èƒ½æµ‹è¯•", 
            "description": "æµ‹è¯•ç³»ç»Ÿåœ¨ä¸­ç­‰è´Ÿè½½ä¸‹çš„æ€§èƒ½",
            "sample_size": 50,
            "datasets": available_datasets,
            "repetitions": 1
        },
        {
            "name": "å¤§è§„æ¨¡å‹åŠ›æµ‹è¯•",
            "description": "æµ‹è¯•ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„æ€§èƒ½å’Œç¨³å®šæ€§",
            "sample_size": 100,
            "datasets": available_datasets,
            "repetitions": 1
        }
    ]
    
    # è¿è¡Œå®éªŒåºåˆ—
    all_results = []
    
    for i, config in enumerate(experiment_configs, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ§ª å®éªŒ {i}/{len(experiment_configs)}: {config['name']}")
        print(f"ğŸ“ æè¿°: {config['description']}")
        print(f"ğŸ“Š é…ç½®: {config['sample_size']} æ ·æœ¬/æ•°æ®é›†, {len(config['datasets'])} æ•°æ®é›†")
        print(f"{'='*70}")
        
        try:
            # åˆ›å»ºå®éªŒé…ç½®æ–‡ä»¶
            config_path = f"experiments/large_scale_config_{i}.json"
            experiment_config = {
                "datasets": config["datasets"],
                "systems": ["intelligent_rag"],
                "metrics": ["retrieval_quality", "answer_quality", "efficiency"],
                "sample_size": config["sample_size"],
                "repetitions": config["repetitions"],
                "timeout": 60
            }
            
            os.makedirs("experiments", exist_ok=True)
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(experiment_config, f, indent=2)
            
            # è¿è¡Œå®éªŒ
            start_time = time.time()
            runner = ExperimentRunner(config_path)
            results = runner.run_experiment()
            end_time = time.time()
            
            if "error" in results:
                print(f"âŒ å®éªŒå¤±è´¥: {results['error']}")
                continue
            
            # åˆ†æç»“æœ
            experiment_analysis = analyze_experiment_results(results, config, end_time - start_time)
            all_results.append(experiment_analysis)
            
            # æ˜¾ç¤ºå®éªŒç»“æœ
            display_experiment_results(experiment_analysis)
            
        except Exception as e:
            print(f"âŒ å®éªŒ {i} æ‰§è¡Œå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # ç»¼åˆåˆ†æ
    if all_results:
        print(f"\n{'='*70}")
        print("ğŸ“Š ç»¼åˆå®éªŒåˆ†æ")
        print(f"{'='*70}")
        
        comprehensive_analysis = analyze_comprehensive_results(all_results)
        display_comprehensive_analysis(comprehensive_analysis)
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        save_comprehensive_report(all_results, comprehensive_analysis)
    
    print(f"\n{'='*70}")
    print("ğŸ‰ å¤§è§„æ¨¡å®éªŒå®Œæˆï¼")
    print("ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨ experiments/results/ ç›®å½•")
    print("ğŸ“„ ç»¼åˆæŠ¥å‘Šä¿å­˜åœ¨ experiments/comprehensive_report.md")
    print(f"{'='*70}")
    
    return 0


def analyze_experiment_results(results: Dict[str, Any], config: Dict[str, Any], duration: float) -> Dict[str, Any]:
    """åˆ†æå•ä¸ªå®éªŒç»“æœ"""
    analysis = {
        "config": config,
        "duration": duration,
        "datasets": {},
        "overall": {}
    }
    
    total_queries = 0
    total_successful = 0
    all_confidences = []
    all_processing_times = []
    all_complexities = []
    
    # åˆ†æå„æ•°æ®é›†ç»“æœ
    for dataset_name, dataset_results in results["results"].items():
        for system_name, system_result in dataset_results.items():
            if "error" in system_result:
                continue
            
            stats = system_result.get("system_stats", {})
            successful = system_result.get("successful_queries", 0)
            total = system_result.get("total_queries", 0)
            
            total_queries += total
            total_successful += successful
            
            if stats.get("avg_confidence"):
                all_confidences.append(stats["avg_confidence"])
            if stats.get("avg_processing_time"):
                all_processing_times.append(stats["avg_processing_time"])
            if stats.get("avg_complexity"):
                all_complexities.append(stats["avg_complexity"])
            
            # æ•°æ®é›†çº§åˆ«åˆ†æ
            analysis["datasets"][dataset_name] = {
                "total_queries": total,
                "successful_queries": successful,
                "success_rate": successful / total if total > 0 else 0,
                "avg_confidence": stats.get("avg_confidence", 0),
                "avg_processing_time": stats.get("avg_processing_time", 0),
                "throughput": stats.get("throughput", 0),
                "avg_complexity": stats.get("avg_complexity", 0)
            }
    
    # æ•´ä½“åˆ†æ
    analysis["overall"] = {
        "total_queries": total_queries,
        "successful_queries": total_successful,
        "success_rate": total_successful / total_queries if total_queries > 0 else 0,
        "avg_confidence": statistics.mean(all_confidences) if all_confidences else 0,
        "avg_processing_time": statistics.mean(all_processing_times) if all_processing_times else 0,
        "total_throughput": total_successful / duration if duration > 0 else 0,
        "avg_complexity": statistics.mean(all_complexities) if all_complexities else 0
    }
    
    # æ€§èƒ½ç­‰çº§è¯„ä¼°
    avg_time = analysis["overall"]["avg_processing_time"]
    if avg_time < 0.1:
        performance_level = "ğŸš€ ä¼˜ç§€"
    elif avg_time < 0.5:
        performance_level = "âœ… è‰¯å¥½"
    elif avg_time < 1.0:
        performance_level = "âš ï¸  ä¸€èˆ¬"
    else:
        performance_level = "âŒ éœ€è¦ä¼˜åŒ–"
    
    analysis["performance_level"] = performance_level
    
    return analysis


def display_experiment_results(analysis: Dict[str, Any]):
    """æ˜¾ç¤ºå®éªŒç»“æœ"""
    config = analysis["config"]
    overall = analysis["overall"]
    
    print(f"\nğŸ“Š å®éªŒç»“æœ: {config['name']}")
    print("-" * 50)
    print(f"æ€»æŸ¥è¯¢æ•°: {overall['total_queries']}")
    print(f"æˆåŠŸæŸ¥è¯¢æ•°: {overall['successful_queries']}")
    print(f"æˆåŠŸç‡: {overall['success_rate']:.1%}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {overall['avg_confidence']:.1%}")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {overall['avg_processing_time']:.3f}ç§’")
    print(f"æ€»ä½“ååé‡: {overall['total_throughput']:.1f} queries/sec")
    print(f"å¹³å‡æŸ¥è¯¢å¤æ‚åº¦: {overall['avg_complexity']:.2f}/5.0")
    print(f"æ€§èƒ½ç­‰çº§: {analysis['performance_level']}")
    
    # å„æ•°æ®é›†è¯¦æƒ…
    print(f"\nğŸ“‹ å„æ•°æ®é›†è¯¦æƒ…:")
    for dataset_name, dataset_stats in analysis["datasets"].items():
        print(f"  {dataset_name}:")
        print(f"    æˆåŠŸç‡: {dataset_stats['success_rate']:.1%}")
        print(f"    ç½®ä¿¡åº¦: {dataset_stats['avg_confidence']:.1%}")
        print(f"    å¤„ç†æ—¶é—´: {dataset_stats['avg_processing_time']:.3f}s")
        print(f"    ååé‡: {dataset_stats['throughput']:.1f} q/s")


def analyze_comprehensive_results(all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç»¼åˆåˆ†ææ‰€æœ‰å®éªŒç»“æœ"""
    analysis = {
        "scalability": {},
        "performance_trends": {},
        "stability": {},
        "recommendations": []
    }
    
    # å¯æ‰©å±•æ€§åˆ†æ
    sample_sizes = [r["config"]["sample_size"] for r in all_results]
    success_rates = [r["overall"]["success_rate"] for r in all_results]
    processing_times = [r["overall"]["avg_processing_time"] for r in all_results]
    throughputs = [r["overall"]["total_throughput"] for r in all_results]
    
    analysis["scalability"] = {
        "sample_sizes": sample_sizes,
        "success_rates": success_rates,
        "processing_times": processing_times,
        "throughputs": throughputs,
        "stability_score": statistics.stdev(success_rates) if len(success_rates) > 1 else 0
    }
    
    # æ€§èƒ½è¶‹åŠ¿åˆ†æ
    if len(processing_times) > 1:
        time_trend = "increasing" if processing_times[-1] > processing_times[0] else "stable"
        throughput_trend = "increasing" if throughputs[-1] > throughputs[0] else "decreasing"
    else:
        time_trend = "stable"
        throughput_trend = "stable"
    
    analysis["performance_trends"] = {
        "processing_time_trend": time_trend,
        "throughput_trend": throughput_trend
    }
    
    # ç¨³å®šæ€§åˆ†æ
    avg_success_rate = statistics.mean(success_rates)
    min_success_rate = min(success_rates)
    
    analysis["stability"] = {
        "avg_success_rate": avg_success_rate,
        "min_success_rate": min_success_rate,
        "stability_rating": "excellent" if min_success_rate > 0.95 else "good" if min_success_rate > 0.8 else "needs_improvement"
    }
    
    # ç”Ÿæˆå»ºè®®
    recommendations = []
    
    if avg_success_rate > 0.95:
        recommendations.append("âœ… ç³»ç»Ÿç¨³å®šæ€§ä¼˜ç§€ï¼Œå¯ä»¥è¿›è¡Œç”Ÿäº§éƒ¨ç½²")
    elif avg_success_rate > 0.8:
        recommendations.append("âš ï¸  ç³»ç»ŸåŸºæœ¬ç¨³å®šï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        recommendations.append("âŒ ç³»ç»Ÿç¨³å®šæ€§éœ€è¦æ”¹è¿›")
    
    if time_trend == "increasing":
        recommendations.append("âš ï¸  å¤„ç†æ—¶é—´éšè´Ÿè½½å¢åŠ ï¼Œéœ€è¦æ€§èƒ½ä¼˜åŒ–")
    else:
        recommendations.append("âœ… å¤„ç†æ—¶é—´ä¿æŒç¨³å®š")
    
    if throughput_trend == "decreasing":
        recommendations.append("âš ï¸  ååé‡éšè´Ÿè½½ä¸‹é™ï¼Œéœ€è¦æ‰©å±•æ€§ä¼˜åŒ–")
    else:
        recommendations.append("âœ… ååé‡è¡¨ç°è‰¯å¥½")
    
    analysis["recommendations"] = recommendations
    
    return analysis


def display_comprehensive_analysis(analysis: Dict[str, Any]):
    """æ˜¾ç¤ºç»¼åˆåˆ†æç»“æœ"""
    scalability = analysis["scalability"]
    stability = analysis["stability"]
    
    print("ğŸ” å¯æ‰©å±•æ€§åˆ†æ:")
    print(f"  æ ·æœ¬è§„æ¨¡: {scalability['sample_sizes']}")
    print(f"  æˆåŠŸç‡: {[f'{r:.1%}' for r in scalability['success_rates']]}")
    print(f"  å¤„ç†æ—¶é—´: {[f'{t:.3f}s' for t in scalability['processing_times']]}")
    print(f"  ååé‡: {[f'{tp:.1f} q/s' for tp in scalability['throughputs']]}")
    print(f"  ç¨³å®šæ€§è¯„åˆ†: {scalability['stability_score']:.3f}")
    
    print(f"\nğŸ“ˆ æ€§èƒ½è¶‹åŠ¿:")
    print(f"  å¤„ç†æ—¶é—´è¶‹åŠ¿: {analysis['performance_trends']['processing_time_trend']}")
    print(f"  ååé‡è¶‹åŠ¿: {analysis['performance_trends']['throughput_trend']}")
    
    print(f"\nğŸ¯ ç¨³å®šæ€§è¯„ä¼°:")
    print(f"  å¹³å‡æˆåŠŸç‡: {stability['avg_success_rate']:.1%}")
    print(f"  æœ€ä½æˆåŠŸç‡: {stability['min_success_rate']:.1%}")
    print(f"  ç¨³å®šæ€§ç­‰çº§: {stability['stability_rating']}")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    for recommendation in analysis["recommendations"]:
        print(f"  {recommendation}")


def save_comprehensive_report(all_results: List[Dict[str, Any]], analysis: Dict[str, Any]):
    """ä¿å­˜ç»¼åˆæŠ¥å‘Š"""
    report_path = "experiments/comprehensive_report.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# æ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿ - å¤§è§„æ¨¡å®éªŒç»¼åˆæŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## å®éªŒæ¦‚è§ˆ\n\n")
        f.write(f"- **å®éªŒæ•°é‡**: {len(all_results)}\n")
        f.write(f"- **æ€»æŸ¥è¯¢æ•°**: {sum(r['overall']['total_queries'] for r in all_results)}\n")
        f.write(f"- **å¹³å‡æˆåŠŸç‡**: {analysis['stability']['avg_success_rate']:.1%}\n\n")
        
        f.write("## è¯¦ç»†å®éªŒç»“æœ\n\n")
        for i, result in enumerate(all_results, 1):
            config = result["config"]
            overall = result["overall"]
            
            f.write(f"### å®éªŒ {i}: {config['name']}\n\n")
            f.write(f"- **æ ·æœ¬è§„æ¨¡**: {config['sample_size']}\n")
            f.write(f"- **æˆåŠŸç‡**: {overall['success_rate']:.1%}\n")
            f.write(f"- **å¹³å‡ç½®ä¿¡åº¦**: {overall['avg_confidence']:.1%}\n")
            f.write(f"- **å¹³å‡å¤„ç†æ—¶é—´**: {overall['avg_processing_time']:.3f}ç§’\n")
            f.write(f"- **ååé‡**: {overall['total_throughput']:.1f} queries/sec\n")
            f.write(f"- **æ€§èƒ½ç­‰çº§**: {result['performance_level']}\n\n")
        
        f.write("## ç»¼åˆåˆ†æ\n\n")
        f.write("### å¯æ‰©å±•æ€§\n\n")
        scalability = analysis["scalability"]
        f.write(f"- **ç¨³å®šæ€§è¯„åˆ†**: {scalability['stability_score']:.3f}\n")
        f.write(f"- **å¤„ç†æ—¶é—´è¶‹åŠ¿**: {analysis['performance_trends']['processing_time_trend']}\n")
        f.write(f"- **ååé‡è¶‹åŠ¿**: {analysis['performance_trends']['throughput_trend']}\n\n")
        
        f.write("### å»ºè®®\n\n")
        for recommendation in analysis["recommendations"]:
            f.write(f"- {recommendation}\n")
        
        f.write("\n---\n\n")
        f.write("*æŠ¥å‘Šç”±æ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*\n")
    
    print(f"ğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
