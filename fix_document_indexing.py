#!/usr/bin/env python3
"""
ä¿®å¤æ–‡æ¡£ç´¢å¼•é—®é¢˜

è§£å†³SQuADç­‰æ•°æ®é›†çš„æ–‡æ¡£æ— æ³•è¢«æ£€ç´¢å™¨æ­£ç¡®ç´¢å¼•çš„é—®é¢˜ã€‚
"""

import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.core.intelligent_adapter import IntelligentAdaptiveRAG
from src.utils.logging import setup_logger


def load_documents_from_dataset(dataset_name: str) -> List[Dict[str, Any]]:
    """
    ä»æ•°æ®é›†åŠ è½½æ–‡æ¡£
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        
    Returns:
        List[Dict]: æ–‡æ¡£åˆ—è¡¨
    """
    logger = setup_logger("DocumentLoader")
    
    try:
        doc_path = Path(f"data/{dataset_name}/documents.json")
        
        if not doc_path.exists():
            logger.warning(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {doc_path}")
            return []
        
        with open(doc_path, 'r', encoding='utf-8') as f:
            documents = json.load(f)
        
        logger.info(f"ä»{dataset_name}åŠ è½½äº†{len(documents)}ä¸ªæ–‡æ¡£")
        return documents
        
    except Exception as e:
        logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {dataset_name}: {str(e)}")
        return []


def test_document_indexing():
    """æµ‹è¯•æ–‡æ¡£ç´¢å¼•åŠŸèƒ½"""
    logger = setup_logger("DocumentIndexTest")
    
    print("ğŸ”§ æµ‹è¯•æ–‡æ¡£ç´¢å¼•åŠŸèƒ½...")
    
    # æµ‹è¯•å„ä¸ªæ•°æ®é›†
    datasets = ["samples", "squad", "synthetic"]
    
    for dataset_name in datasets:
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®é›†: {dataset_name}")
        print("-" * 40)
        
        # åŠ è½½æ–‡æ¡£
        documents = load_documents_from_dataset(dataset_name)
        
        if not documents:
            print(f"âŒ {dataset_name}: æ²¡æœ‰æ–‡æ¡£å¯åŠ è½½")
            continue
        
        print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        
        # æ˜¾ç¤ºæ–‡æ¡£æ ·ä¾‹
        for i, doc in enumerate(documents[:3]):
            print(f"  æ–‡æ¡£ {i+1}:")
            print(f"    ID: {doc.get('id', 'N/A')}")
            print(f"    æ ‡é¢˜: {doc.get('title', 'N/A')[:50]}...")
            print(f"    å†…å®¹é•¿åº¦: {len(doc.get('content', ''))}")
        
        # æµ‹è¯•RAGç³»ç»Ÿæ˜¯å¦èƒ½æ­£ç¡®å¤„ç†è¿™äº›æ–‡æ¡£
        try:
            rag = IntelligentAdaptiveRAG()
            
            # æ‰‹åŠ¨æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨ (è¿™æ˜¯é—®é¢˜æ‰€åœ¨)
            print(f"  ğŸ”§ æ‰‹åŠ¨æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨...")
            
            # ä¸ºæ¯ä¸ªæ£€ç´¢å™¨æ·»åŠ æ–‡æ¡£
            for retriever_name in ['dense', 'sparse', 'hybrid']:
                retriever = getattr(rag, f'{retriever_name}_retriever', None)
                if retriever and hasattr(retriever, 'add_documents'):
                    retriever.add_documents(documents)
                    print(f"    âœ… å·²æ·»åŠ æ–‡æ¡£åˆ° {retriever_name} æ£€ç´¢å™¨")
            
            # æµ‹è¯•æŸ¥è¯¢
            test_queries = [
                "What is machine learning?",
                "Notre Dame",
                "artificial intelligence"
            ]
            
            for query in test_queries:
                print(f"  ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
                result = rag.process_query(query)
                print(f"    æ£€ç´¢åˆ°æ–‡æ¡£æ•°: {len(result.retrieved_documents)}")
                print(f"    ç½®ä¿¡åº¦: {result.overall_confidence:.2%}")
                
                if len(result.retrieved_documents) > 0:
                    print(f"    âœ… æ£€ç´¢æˆåŠŸ")
                else:
                    print(f"    âŒ æ£€ç´¢å¤±è´¥")
        
        except Exception as e:
            print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")


def create_enhanced_rag_with_documents():
    """åˆ›å»ºå¢å¼ºçš„RAGç³»ç»Ÿï¼Œé¢„åŠ è½½æ‰€æœ‰æ•°æ®é›†æ–‡æ¡£"""
    logger = setup_logger("EnhancedRAG")
    
    print("ğŸš€ åˆ›å»ºå¢å¼ºçš„RAGç³»ç»Ÿ...")
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = IntelligentAdaptiveRAG()
    
    # åŠ è½½æ‰€æœ‰æ•°æ®é›†çš„æ–‡æ¡£
    all_documents = []
    datasets = ["samples", "squad", "synthetic"]
    
    for dataset_name in datasets:
        documents = load_documents_from_dataset(dataset_name)
        if documents:
            # ä¸ºæ–‡æ¡£æ·»åŠ æ•°æ®é›†æ ‡è¯†
            for doc in documents:
                doc['dataset'] = dataset_name
            all_documents.extend(documents)
            print(f"âœ… åŠ è½½ {dataset_name}: {len(documents)} ä¸ªæ–‡æ¡£")
    
    print(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(all_documents)} ä¸ªæ–‡æ¡£")
    
    # æ‰‹åŠ¨æ·»åŠ æ–‡æ¡£åˆ°æ‰€æœ‰æ£€ç´¢å™¨
    print("ğŸ”§ æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨...")
    
    try:
        # æ·»åŠ åˆ°ç¨ å¯†æ£€ç´¢å™¨
        if hasattr(rag.dense_retriever, 'add_documents'):
            rag.dense_retriever.add_documents(all_documents)
            print("âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°ç¨ å¯†æ£€ç´¢å™¨")
        
        # æ·»åŠ åˆ°ç¨€ç–æ£€ç´¢å™¨
        if hasattr(rag.sparse_retriever, 'add_documents'):
            rag.sparse_retriever.add_documents(all_documents)
            print("âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°ç¨€ç–æ£€ç´¢å™¨")
        
        # æ·»åŠ åˆ°æ··åˆæ£€ç´¢å™¨
        if hasattr(rag.hybrid_retriever, 'add_documents'):
            rag.hybrid_retriever.add_documents(all_documents)
            print("âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°æ··åˆæ£€ç´¢å™¨")
    
    except Exception as e:
        print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
        return None
    
    return rag


def run_enhanced_experiment():
    """è¿è¡Œå¢å¼ºçš„å®éªŒ"""
    print("ğŸ§ª è¿è¡Œå¢å¼ºå®éªŒ...")
    
    # åˆ›å»ºå¢å¼ºçš„RAGç³»ç»Ÿ
    rag = create_enhanced_rag_with_documents()
    
    if not rag:
        print("âŒ æ— æ³•åˆ›å»ºå¢å¼ºRAGç³»ç»Ÿ")
        return
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        {"query": "What is machine learning?", "expected_dataset": "samples"},
        {"query": "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?", "expected_dataset": "squad"},
        {"query": "What are the applications of deep learning?", "expected_dataset": "synthetic"},
        {"query": "How does neural networks work?", "expected_dataset": "synthetic"},
        {"query": "What is the Grotto at Notre Dame?", "expected_dataset": "squad"}
    ]
    
    print(f"\nğŸ” æµ‹è¯• {len(test_queries)} ä¸ªæŸ¥è¯¢...")
    print("=" * 60)
    
    results = []
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case["query"]
        expected_dataset = test_case["expected_dataset"]
        
        print(f"\næŸ¥è¯¢ {i}: {query}")
        print(f"æœŸæœ›æ•°æ®é›†: {expected_dataset}")
        print("-" * 40)
        
        try:
            result = rag.process_query(query)
            
            # åˆ†æç»“æœ
            doc_count = len(result.retrieved_documents)
            confidence = result.overall_confidence
            
            print(f"æ£€ç´¢åˆ°æ–‡æ¡£æ•°: {doc_count}")
            print(f"ç½®ä¿¡åº¦: {confidence:.2%}")
            
            if doc_count > 0:
                print("å‰3ä¸ªæ–‡æ¡£:")
                for j, doc in enumerate(result.retrieved_documents[:3]):
                    dataset = getattr(doc, 'dataset', 'unknown')
                    score = getattr(doc, 'final_score', 0)
                    print(f"  {j+1}. æ•°æ®é›†: {dataset}, åˆ†æ•°: {score:.3f}")
                
                # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†æœŸæœ›æ•°æ®é›†çš„æ–‡æ¡£
                found_datasets = set()
                for doc in result.retrieved_documents:
                    dataset = getattr(doc, 'dataset', 'unknown')
                    found_datasets.add(dataset)
                
                if expected_dataset in found_datasets:
                    print(f"âœ… æˆåŠŸæ‰¾åˆ°æ¥è‡ª {expected_dataset} çš„æ–‡æ¡£")
                else:
                    print(f"âš ï¸  æœªæ‰¾åˆ°æ¥è‡ª {expected_dataset} çš„æ–‡æ¡£")
                    print(f"å®é™…æ‰¾åˆ°çš„æ•°æ®é›†: {found_datasets}")
            else:
                print("âŒ æ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£")
            
            results.append({
                "query": query,
                "expected_dataset": expected_dataset,
                "doc_count": doc_count,
                "confidence": confidence,
                "success": doc_count > 0
            })
        
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}")
            results.append({
                "query": query,
                "expected_dataset": expected_dataset,
                "doc_count": 0,
                "confidence": 0.0,
                "success": False,
                "error": str(e)
            })
    
    # æ€»ç»“ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š å®éªŒç»“æœæ€»ç»“:")
    print("=" * 60)
    
    successful_queries = sum(1 for r in results if r["success"])
    total_queries = len(results)
    avg_confidence = sum(r["confidence"] for r in results) / total_queries
    avg_doc_count = sum(r["doc_count"] for r in results) / total_queries
    
    print(f"æˆåŠŸæŸ¥è¯¢: {successful_queries}/{total_queries} ({successful_queries/total_queries*100:.1f}%)")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2%}")
    print(f"å¹³å‡æ£€ç´¢æ–‡æ¡£æ•°: {avg_doc_count:.1f}")
    
    if successful_queries == total_queries:
        print("ğŸ‰ æ‰€æœ‰æŸ¥è¯¢éƒ½æˆåŠŸï¼æ–‡æ¡£ç´¢å¼•é—®é¢˜å·²è§£å†³ï¼")
    else:
        print("âš ï¸  ä»æœ‰éƒ¨åˆ†æŸ¥è¯¢å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿ - æ–‡æ¡£ç´¢å¼•ä¿®å¤")
    print("=" * 60)
    
    # æ­¥éª¤1: æµ‹è¯•æ–‡æ¡£ç´¢å¼•
    test_document_indexing()
    
    print("\n" + "=" * 60)
    
    # æ­¥éª¤2: è¿è¡Œå¢å¼ºå®éªŒ
    results = run_enhanced_experiment()
    
    print("\nğŸ¯ ä¿®å¤å®Œæˆï¼")
    
    return results


if __name__ == "__main__":
    main()
