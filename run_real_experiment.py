#!/usr/bin/env python3
"""
çœŸå®æ•°æ®é›†å®éªŒè„šæœ¬

åœ¨çœŸå®æ•°æ®é›†ä¸Šè¿è¡Œæ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿå®éªŒï¼Œ
åŒ…æ‹¬SQuADå’Œåˆæˆæ•°æ®é›†çš„å®Œæ•´è¯„ä¼°ã€‚
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿ - çœŸå®æ•°æ®é›†å®éªŒ")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒå’Œæ•°æ®...")
    try:
        from src.core.intelligent_adapter import IntelligentAdaptiveRAG
        from experiments.experiment_runner import ExperimentRunner
        
        # æ£€æŸ¥æ•°æ®é›†
        data_dir = Path("data")
        available_datasets = []
        
        for dataset in ["samples", "squad", "synthetic"]:
            dataset_path = data_dir / dataset / "queries.json"
            if dataset_path.exists():
                available_datasets.append(dataset)
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    queries = json.load(f)
                print(f"  âœ… {dataset}: {len(queries)}ä¸ªæŸ¥è¯¢")
            else:
                print(f"  âŒ {dataset}: æ•°æ®é›†ä¸å­˜åœ¨")
        
        if not available_datasets:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†")
            return 1
            
        print(f"âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œå¯ç”¨æ•°æ®é›†: {available_datasets}")
        
    except ImportError as e:
        print(f"âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
        return 1
    
    # è¿è¡Œå¤šæ•°æ®é›†å®éªŒ
    print(f"\nğŸ§ª è¿è¡Œå¤šæ•°æ®é›†å®éªŒ...")
    try:
        # åˆ›å»ºå®éªŒé…ç½®
        experiment_config = {
            "datasets": available_datasets,
            "systems": ["intelligent_rag"],
            "metrics": ["retrieval_quality", "answer_quality", "efficiency"],
            "sample_size": 20,  # æ¯ä¸ªæ•°æ®é›†æµ‹è¯•20ä¸ªæŸ¥è¯¢
            "repetitions": 1,
            "timeout": 30
        }
        
        # ä¿å­˜é…ç½®
        config_path = "experiments/real_experiment_config.json"
        os.makedirs("experiments", exist_ok=True)
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(experiment_config, f, indent=2)
        
        # è¿è¡Œå®éªŒ
        print(f"ğŸ“Š å®éªŒé…ç½®:")
        print(f"  - æ•°æ®é›†: {', '.join(available_datasets)}")
        print(f"  - æ¯ä¸ªæ•°æ®é›†æ ·æœ¬æ•°: {experiment_config['sample_size']}")
        print(f"  - æ€»æŸ¥è¯¢æ•°: {len(available_datasets) * experiment_config['sample_size']}")
        
        runner = ExperimentRunner(config_path)
        start_time = time.time()
        results = runner.run_experiment()
        end_time = time.time()
        
        if "error" in results:
            print(f"âŒ å®éªŒå¤±è´¥: {results['error']}")
            return 1
        
        print("âœ… çœŸå®æ•°æ®é›†å®éªŒå®Œæˆ")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print(f"\nğŸ“ˆ å®éªŒç»“æœè¯¦æƒ…:")
        print("-" * 60)
        print(f"æ€»å®éªŒæ—¶é—´: {end_time - start_time:.2f}ç§’")
        
        total_queries = 0
        total_successful = 0
        all_confidences = []
        all_processing_times = []
        
        for dataset_name, dataset_results in results["results"].items():
            print(f"\nğŸ“Š æ•°æ®é›†: {dataset_name}")
            print("-" * 30)
            
            for system_name, system_result in dataset_results.items():
                if "error" in system_result:
                    print(f"  {system_name}: âŒ é”™è¯¯ - {system_result['error']}")
                    continue
                
                stats = system_result.get("system_stats", {})
                successful = system_result.get("successful_queries", 0)
                total = system_result.get("total_queries", 0)
                
                total_queries += total
                total_successful += successful
                
                if stats.get("avg_confidence"):
                    all_confidences.append(stats["avg_confidence"])
                if stats.get("avg_processing_time"):
                    all_processing_times.append(stats["avg_processing_time"])
                
                print(f"  ğŸ“‹ {system_name}:")
                print(f"    - æˆåŠŸæŸ¥è¯¢: {successful}/{total} ({successful/total*100:.1f}%)")
                print(f"    - å¹³å‡ç½®ä¿¡åº¦: {stats.get('avg_confidence', 0):.1%}")
                print(f"    - å¹³å‡å¤„ç†æ—¶é—´: {stats.get('avg_processing_time', 0):.3f}ç§’")
                print(f"    - ååé‡: {stats.get('throughput', 0):.1f} queries/sec")
                print(f"    - å¹³å‡æŸ¥è¯¢å¤æ‚åº¦: {stats.get('avg_complexity', 0):.2f}/5.0")
        
        # æ€»ä½“ç»Ÿè®¡
        print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½ç»Ÿè®¡:")
        print("-" * 30)
        print(f"æ€»æŸ¥è¯¢æ•°: {total_queries}")
        print(f"æˆåŠŸæŸ¥è¯¢æ•°: {total_successful}")
        print(f"æ€»ä½“æˆåŠŸç‡: {total_successful/total_queries*100:.1f}%")
        
        if all_confidences:
            avg_confidence = sum(all_confidences) / len(all_confidences)
            print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1%}")
        
        if all_processing_times:
            avg_time = sum(all_processing_times) / len(all_processing_times)
            total_throughput = total_successful / sum(all_processing_times) if sum(all_processing_times) > 0 else 0
            print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.3f}ç§’")
            print(f"æ€»ä½“ååé‡: {total_throughput:.1f} queries/sec")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: experiments/results/{results['experiment_id']}_results.json")
        print(f"ğŸ“„ å®éªŒæŠ¥å‘Šä¿å­˜åœ¨: experiments/results/{results['experiment_id']}_report.md")
        
    except Exception as e:
        print(f"âŒ å®éªŒè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # æ€§èƒ½åˆ†æ
    print(f"\nğŸ“Š æ€§èƒ½åˆ†æ:")
    print("-" * 30)
    
    if all_processing_times and len(all_processing_times) > 1:
        import statistics
        
        min_time = min(all_processing_times)
        max_time = max(all_processing_times)
        median_time = statistics.median(all_processing_times)
        std_time = statistics.stdev(all_processing_times)
        
        print(f"å¤„ç†æ—¶é—´åˆ†å¸ƒ:")
        print(f"  - æœ€å°å€¼: {min_time:.3f}ç§’")
        print(f"  - æœ€å¤§å€¼: {max_time:.3f}ç§’")
        print(f"  - ä¸­ä½æ•°: {median_time:.3f}ç§’")
        print(f"  - æ ‡å‡†å·®: {std_time:.3f}ç§’")
        
        # æ€§èƒ½ç­‰çº§è¯„ä¼°
        if avg_time < 0.1:
            performance_level = "ğŸš€ ä¼˜ç§€ (< 0.1s)"
        elif avg_time < 0.5:
            performance_level = "âœ… è‰¯å¥½ (< 0.5s)"
        elif avg_time < 1.0:
            performance_level = "âš ï¸  ä¸€èˆ¬ (< 1.0s)"
        else:
            performance_level = "âŒ éœ€è¦ä¼˜åŒ– (> 1.0s)"
        
        print(f"æ€§èƒ½ç­‰çº§: {performance_level}")
    
    # ä¸‹ä¸€æ­¥å»ºè®®
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("-" * 30)
    
    if total_successful / total_queries > 0.9:
        print("âœ… ç³»ç»Ÿç¨³å®šæ€§è‰¯å¥½ï¼Œå¯ä»¥è¿›è¡Œæ›´å¤§è§„æ¨¡å®éªŒ")
        print("ğŸ”¬ å»ºè®®: å¢åŠ æ ·æœ¬æ•°é‡åˆ°100-500ä¸ªæŸ¥è¯¢")
        print("ğŸ“Š å»ºè®®: å®ç°æ›´å¤šåŸºå‡†æ–¹æ³•è¿›è¡Œå¯¹æ¯”")
    elif total_successful / total_queries > 0.7:
        print("âš ï¸  ç³»ç»ŸåŸºæœ¬ç¨³å®šï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        print("ğŸ”§ å»ºè®®: æ£€æŸ¥å¤±è´¥æŸ¥è¯¢çš„åŸå› ")
        print("âš™ï¸  å»ºè®®: è°ƒä¼˜ç³»ç»Ÿå‚æ•°")
    else:
        print("âŒ ç³»ç»Ÿç¨³å®šæ€§éœ€è¦æ”¹è¿›")
        print("ğŸ› å»ºè®®: è°ƒè¯•ç³»ç»Ÿé”™è¯¯")
        print("ğŸ”§ å»ºè®®: ä¼˜åŒ–æ ¸å¿ƒç»„ä»¶")
    
    print("ğŸ“ å»ºè®®: å¼€å§‹æ’°å†™æŠ€æœ¯è®ºæ–‡")
    print("ğŸš€ å»ºè®®: å‡†å¤‡å¼€æºå‘å¸ƒ")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ çœŸå®æ•°æ®é›†å®éªŒå®Œæˆï¼")
    print("ğŸ¯ æ™ºèƒ½è‡ªé€‚åº”RAGç³»ç»Ÿå·²åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æ€§èƒ½ï¼")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
